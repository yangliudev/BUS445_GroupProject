---
title: "BUS445"
output: html_document
date: "2024-11-07"
---

# Summary

Did some investigation for which variables are important. Removed NA missing values (4 total). Renamed some NULLS values to better explain why there were NULL. Did some feature engineering to look at continents, holiday, and seasonal results. I chose to remove the reservation_status as the data is mainly after a cancellation/non cancellation has occurred. 

Used random forests, decision tree, and logistic regression primarily. Random forests found that the top 5 important variables are: deposit_type NonRefundable,country: Portugal, lead_time, total_of_special_requests, and previous_cancellations. Decision trees found that the top 5 important variables are: 	deposit_type Non Refundable,		
country Portugal,	previous_cancellation, market_segment: Online TA and lead_time. Quite similar due to random forests being the average of a bunch of decision trees. 

Logistic regression was quite different and found company204, assigned_room_typeI, agent252, company321, and required_car_parking_spaces important. Probably due to logistic regression being linear function compared to the other methods.

Did 2 Neural networks since can't do one on all of the variables. One on the important variables from the random forest and decision tree. One on the important variables from logistic regression. The nn from the variables from the tree methods (RF and DT) performed pretty well. Important variables are deposit_type Non Refund, previous_cancellations, market_segment Offline TA/TO, market_segment Direct, and country PRT. 


The nn on the variables from logistic regression performed pretty poorly. (Probably due to how it could only run with one hidden layer.) Important variables found are agent7, agent492, required_car_parking_spaces,company48,agent28.

In terms of accuracy on the test set, 
Logistic Regression (84%) > Decision Tree (81%)  > Neural Network Tree Variables (79%) > Random Forest (75%) > Neural Network Logistic Regression Variables (71%).

Still need to do plots on the important variables. Not sure if should do cross validation as it takes a while to run vs train test sets. Did a bit of train test sets at the end. Seems to perform better.

```{r setup, include=FALSE}
library(dplyr)
library(nnet)
library(rpart.plot)
library(tibble)
library(rpart)
library(caret)
library(corrplot)
library(countrycode)
library(ranger)
```

# Exploration of Missing Values
```{r}
set.seed(123)
data=read.csv("hotel_bookings.csv")
originalData=data
#Checking for missing values (NA). Observed 4 missing values in the children column.
data[rowSums(is.na(data))>0,]
```
```{r}
#Removing these 4 instances as there is a lot of observations
data=na.omit(data)
```

Contingency table of all the columns
```{r}
#lapply(data,table) Commented out as it's too big of a print.
```
It's observed that there are NULL values in the data. The columns with NULL values are company, agent, and country.  
```{r}
colSums(data=="NULL")
```
The contigency table for the company feature. 
```{r}
#table(data$company) Commented out as it's too big of a print.
```
It is observed that the most common element is the NULL value with 112589 observations which is much more than 50% of the data.
This is most likely due to a majority of the hotel bookings not be associated with a company booking. As a result, this implys that 
the NULL values are important so they will be renamed to "No Company"
```{r}
data=data%>%mutate(company=ifelse(company=="NULL","No Company",company))
```
The agent feature has 16338 NULL values. As the agent number is related to the distribution channel of the booking, we will investigate the distribution channel.
```{r}
#table(data$agent) Commented out as it's too big of a print.
```

```{r}
agentNullData=data%>% filter(agent=="NULL")
#table(agentNullData$agent,agentNullData$distribution_channel) Commented out as it's too big of a print.
```

Of the 16338 NULL values in the agent field, 13168 (5543+7625) of them belong to the corporate and direct distribution channels which have no agents as they directly contact the hotel for the booking. We will fill these with "No Travel Agency" as they don't use any travel agency. There is 3167 NULL values with TA/TO distribution channels. We will fill in these with "TA/TO No Agent Number" as they have travel agents but have no agent id. The remaining 3 NULL values will be removed as they are only 3 of them.

```{r}
data=data%>%mutate(agent=ifelse(distribution_channel %in% c("Corporate","Direct") & agent=='NULL','No Travel Agency',agent))
data=data%>%mutate(agent=ifelse(distribution_channel=="TA/TO" & agent=="NULL","TA/TO No Agent Number",agent))
data=data%>%filter(agent!="NULL")
```

Looking at the Contingency table of the country column we see that there is 488 NULL values.
```{r}
#table(data$country) Commented out as it's too big of a print.
```

```{r}
countryNulldata=data%>% filter(country=="NULL")
x=table(countryNulldata$country,countryNulldata$agent)
#x["NULL",] Commented out as it's too big of a print.
```
It is observed that majority of the observations with NULL for countries also had no agents which are now "No Travel Agency" and "TA/TO No Agent Number". We will fill these with countries with "Unknown". For all the other NULL countries, we will remove them as there is a small amount of them.

```{r}
data=data%>%mutate(country=ifelse(agent %in% c("No Travel Agency","TA/TO No Agent Number") & country=='NULL','Unknown',country))
data=data%>%filter(data$country!="NULL")
```


```{r}
#lapply(data,table)
```

It is observed that there is 1168 undefined columns in the meal feature. As the other options are BB (Bed and Breakfast), FB(Full Board), HB(Half Board), and SC (Self Catering) it is observed that there is no option for no meal services. As a result, we will fill these undefined values with "Other"

```{r}
data=data%>%mutate(meal=ifelse(meal=='Undefined','Other',meal))
table(data$meal)
```

```{r}
head(data)
write.csv(data,"data.csv",row.names = FALSE) # Writing out for easier factor conversion
```
```{r}
data=read.csv("data.csv",stringsAsFactors = TRUE)
data$is_canceled=as.factor(data$is_canceled)
file.remove("data.csv") 
```
# Correlation Exploration 
```{r}
library(corrplot)
numericData=data[sapply(data,is.numeric)]
corr=cor(numericData)
corrplot(corr,method="color")
```
As their isn't any highly correlated columns, no columns will be removed.


# Creating New Features

Binning the lead time into quartiles
```{r}

q1LeadTime=quantile(data$lead_time,0.25)
q2LeadTime=quantile(data$lead_time,0.50)
q3LeadTime=quantile(data$lead_time,0.75)
data$lead_timeCategories=cut(data$lead_time,breaks=c(-Inf,q1LeadTime,q2LeadTime,q3LeadTime,Inf),labels=c("Very Low Lead Time", "Below Average Lead Time", "Above Average Lead Time", "High Lead Time"))
```

Making a continent Column
```{r}
data$Continent=countrycode(data$country, origin = "iso3c", destination = "continent")
southAmerica=c("ARG", "BRA", "CHL", "PER", "COL", "VEN", "SUR", "ECU", "GUY", "PRY", "BOL", "GUY")

#Manually fixing continent values that the country code couldn't define

#South America is linked together as Americas with North America
data$Continent=ifelse(data$country %in% southAmerica & data$Continent == "Americas","South America",data$Continent) 
data$Continent=ifelse(data$country == "ATF", "None",data$Continent)  #French South Territories isn't associated with a continent
data$Continent=ifelse(data$country == "CN","Asia",data$Continent) #China
data$Continent=ifelse(data$country == "TMP","Asia",data$Continent) #East Timor, part of ASIA
data$Continent=ifelse(data$country == "UMI","None",data$Continent) #United States Minor Outlying Islands isn't associated with a continent
data$Continent=ifelse(data$country == "Unknown","Unknown",data$Continent) 
```

Making a holiday seasons column (Summer, Chirstmas, New years)
```{r}
data$ArrivalHolidaySeason=cut(data$arrival_date_week_number,breaks=c(-Inf,1,20,26,47,51,Inf),labels=c("New Year","Regular","Summer","Regular","Chirstmas","New Year"))
```

Making a seasonal column
```{r}
data=data%>%mutate(ArrivalSeason=case_when(
    arrival_date_month %in% c("December", "January", "February") ~ "Winter",
    arrival_date_month %in% c("March", "April", "May") ~ "Spring",
    arrival_date_month %in% c("June", "July", "August") ~ "Summer",
    arrival_date_month %in% c("September", "October", "November") ~ "Fall")
  )
data$ArrivalSeason=as.factor(data$ArrivalSeason)
```




```{r}
originalData=data#Before removing columns stored original with features engineered for later use.

data=subset(data,select=-reservation_status) #Dropping variables that are observed after a hotel booking is finalized (Canceled, No Show, etc)
data=subset(data,select=-reservation_status_date)

data=subset(data,select=-arrival_date_week_number)#Dropping arrival week number as I used it to create the Seasonal columns
```

# Splitting the data for ML
```{r}
data$is_canceled=as.factor(data$is_canceled)
data$Continent=as.factor(data$Continent)
partition=createDataPartition(data$is_canceled,p=0.75,list=FALSE)
data_train=data[partition,]
data_test=data[-partition,]
```




# Random Forest

Used cross validation to tune for parameters for RF. 
```{r}
#Commented out as it takes a while to run. The final values used for the model were mtry = 6, splitrule = gini and min.node.size = 10.
#gridRF=expand.grid(mtry=round(sqrt(ncol(data_train))),splitrule="gini",min.node.size= c(1, 5, 10, 20, 50))
#control=trainControl(method="cv",number=5,verboseIter=TRUE)
#model=train(is_canceled~.,data=data_train,method="ranger",tuneGrid=gridRF,trControl=control,importance = "impurity",num.trees=1000)
#varImp(model)

```

```{r}
gridRF=expand.grid(mtry=6,splitrule="gini",min.node.size=10)
control=trainControl(method="cv",number=5)
rfmodel=train(is_canceled~.,data=data_train,method="ranger",tuneGrid=gridRF,trControl=control,importance = "impurity",num.trees=1000)
```
Accuracy
```{r}
rf_preds=predict(rfmodel,newdata=data_test)
mean(rf_preds==data_test$is_canceled)
```
Confusion Matrix
```{r}
table(rf_preds,data_test$is_canceled)
```
Variable Importance
```{r}
rfImportance=varImp(rfmodel)
Top5RfImportance=rfImportance$importance%>%as.data.frame()%>%rownames_to_column("Feature") %>% arrange(desc(Overall))%>%head(5)
Top5RfImportance
#Found Deposit type:Non refundable, country:Portugal, lead_time, total of special requests, and previous_cancellations and lead_time important
```

Variable Importance Plot
```{r}
ggplot(data=Top5RfImportance,mapping=aes(x=Overall,y= reorder(Feature, Overall)))+geom_bar(stat="identity",fill="blue")+scale_y_discrete(labels=c("Previous Cancellations","Total of Special Requests","Lead Time","Country: Portugal","Non Refundable Deposit"))+xlab("Importance")+ylab("Variables")+ggtitle("Most Important Variables from the Random Forest Model")
```

# Decision Tree 
As with Random Forests, used cross validation to tune the parameters.
```{r}
#Commented out as it takes a while to tune for the parameters. The final value used for the model was cp = 0.01
#tune_gridTree=expand.grid(cp=seq(0.01,0.1, by=0.01))
#train_controlTree=trainControl(method="cv",number=5,verboseIter=TRUE)
#TreeModel=train(is_canceled~.,data=data_train,method="rpart",trControl=train_controlTree, tuneGrid = tune_gridTree)
```

```{r}
tune_gridTree=expand.grid(cp=0.01)
train_controlTree=trainControl(method="cv",number=5)
TreeModel=train(is_canceled~.,data=data_train,method="rpart",trControl=train_controlTree, tuneGrid = tune_gridTree)
Treepreds=predict(TreeModel,newdata=data_test)
```
Accuracy
```{r}
mean(Treepreds==data_test$is_canceled)
```
Confusion Matrix
```{r}
table(Treepreds,data_test$is_canceled)
```

```{r}
TreeImportance=varImp(TreeModel)
```

Important Variables
```{r}
Top5TreeImportance=TreeImportance$importance%>%as.data.frame()%>%rownames_to_column("Feature") %>% arrange(desc(Overall))%>%head(5)
Top5TreeImportance
#Found Deposity type:Non refundable, country:Portugal, previous_cancellations, Market Segment:Online TA, and lead_time important
```

Important Variables plot
```{r}
ggplot(data=Top5TreeImportance,mapping=aes(x=Overall,y= reorder(Feature, Overall)))+geom_bar(stat="identity",fill="blue")+scale_y_discrete(labels=c("Lead Time","Market Segment: Online TA","Previous Cancellation Count","Country: Portugal","Non Refundable Deposit"))+xlab("Importance")+ylab("Variables")+ggtitle("Most Important Variables from the Decision Tree Model")
```

# Logistic Regression

Was gonna use glm but it wouldn't run. So used multinom instead. Takes a while to run.
```{r}
train_control=trainControl(method="cv",number=5)
LogitModel=train(is_canceled~.,data=data_train,method="multinom",trControl=train_control)
Logitpreds=predict(LogitModel,newdata=data_test)
```

Confusion Matrix
```{r}
table(Logitpreds,data_test$is_canceled)
```
Accuracy
```{r}
mean(Logitpreds==data_test$is_canceled)
```

Important variables
```{r}
lgImportance=varImp(LogitModel)
Top5LgImportance=lgImportance$importance%>%as.data.frame()%>%rownames_to_column("Feature") %>% arrange(desc(Overall))%>%head(5)
Top5LgImportance
#Found company204, assigned_room_typeI, agent252, company321, and required_car_parking_spaces important
```

Important Variables Plot
```{r}
ggplot(data=Top5LgImportance,mapping=aes(x=Overall,y= reorder(Feature, Overall)))+geom_bar(stat="identity",fill="blue")+scale_y_discrete(labels=c("Required Parking Spots","Company 321","Agent 252","Assigned Room Type I","Company 204"))+xlab("Importance")+ylab("Variables")+ggtitle("Most Important Variables from the Logistic Regression Model")
```

Very different result from random forests and decision tree. Probably due to random forests struggling with linear relationships.

# Neural Net on variables from tree methods (RF and DT)

Can't use all the variables so used the ones that random forest and decision tree found important. Used CV to tune for parameters.
```{r}
#size = 5 and decay = 0.1. Commneted out as it takes too long to compute
#nn_trainControl=trainControl(method="cv",number=5,verboseIter = TRUE)
#nn_tuneGrid=expand.grid(size=c(1,2,3,5,10), decay = c(0.1, 0.2, 0.3))
#nnModel=train(is_canceled~lead_time+deposit_type+country+market_segment+previous_cancellations,data=data_train,method="nnet",trControl=nn_trainControl,tuneGrid=nn_tuneGrid)

#only using the variables found as important in the previous sections as it gets too computationally complex
nn_trainControl=trainControl(method="cv",number=5)
nn_tuneGrid=expand.grid(size=5, decay = 0.1)
nnModel=train(is_canceled~lead_time+deposit_type+country+market_segment+previous_cancellations,data=data_train,method="nnet",trControl=nn_trainControl,tuneGrid=nn_tuneGrid)
nnPreds=predict(nnModel,newdata=data_test)
```

Confusion Matrix
```{r}
table(nnPreds,data_test$is_canceled)
```

Accuracy
```{r}
mean(nnPreds==data_test$is_canceled)
```
Important variables
```{r}
nnImportance=varImp(nnModel)
Top5nnImportance=nnImportance$importance%>%as.data.frame()%>%rownames_to_column("Feature") %>% arrange(desc(Overall))%>%head(5)
Top5nnImportance
#Found deposit_typeNon Refund, previous_cancellations, market_segmentOffline TA/TO, market_segmentDirect, countryPRT important
```

Important Variables Plot
```{r}
ggplot(data=Top5nnImportance,mapping=aes(x=Overall,y= reorder(Feature, Overall)))+geom_bar(stat="identity",fill="blue")+scale_y_discrete(labels=c("Country Portugal","Market Segment Direct","Market Segment Offline TA/TO","Previous Cancellation","Deposit Type No Refunds"))+xlab("Importance")+ylab("Variables")+ggtitle("Important Variables from NN on the Important vars from Trees")
```

# Neural Net on Important variables from Logistic Regression
```{r}
#The final values used for the model were size = 1 and decay = 0.1. Commented out as it took too long to run
#NN_trainControl=trainControl(method="cv",number=5,verboseIter = TRUE)
#NN_tuneGrid=expand.grid(size=c(1,2), decay = c(0.1, 0.2, 0.3))
#NNModel=train(is_canceled~lead_time+deposit_type+country+market_segment+previous_cancellations,data=data_train,method="nnet",trControl=NN_trainControl,tuneGrid=NN_tuneGrid, trace = FALSE)
#NNModel
```


```{r}
NN_trainControl=trainControl(method="cv",number=5)
NN_tuneGrid=expand.grid(size=1, decay = 0.1)
NNModel=train(is_canceled~company+agent+assigned_room_type+required_car_parking_spaces,data=data_train,method="nnet",trControl=NN_trainControl,tuneGrid=NN_tuneGrid, trace = FALSE)
NNPreds=predict(NNModel,newdata=data_test)
```
Confusion Matrix
```{r}
table(NNPreds,data_test$is_canceled)
```

Accuracy
```{r}
mean(NNPreds==data_test$is_canceled)
```
Important variables
```{r}
NNImportance=varImp(NNModel)
Top5NNImportance=NNImportance$importance%>%as.data.frame()%>%rownames_to_column("Feature") %>% arrange(desc(Overall))%>%head(5)
Top5NNImportance
#Found agent7, agent492, required_car_parking_spaces,company48,agent28 to be important
```

Important Variables Plot
```{r}
ggplot(data=Top5NNImportance,mapping=aes(x=Overall,y= reorder(Feature, Overall)))+geom_bar(stat="identity",fill="blue")+scale_y_discrete(labels=c("Agent 28","Company 48","Required Parking Spaces","Agent 492","Agent 7"))+xlab("Importance")+ylab("Variables")+ggtitle("Important Variables from the NN on Important Vars from LG")
```

# Train test split Only. No cross validation. Computationally faster. Better Results too. Will explore more tmr.
```{r}
rg=ranger(is_canceled~.,data=data_train,num.trees=1000,importance="impurity")
```


```{r}
rgPreds=predict(rg,data=data_test)
mean(rgPreds$predictions==data_test$is_canceled)
```

```{r}
importance(rg)
```

```{r}
table(rgPreds$predictions,data_test$is_canceled)
```

```{r}
tree=rpart(is_canceled~.,data=data_train, method = "class")
```

```{r}
treePreds=predict(tree,newdata=data_test,type="class")
mean(treePreds==data_test$is_canceled)
```

```{r}
table(treePreds,data_test$is_canceled)
```
# Logistic Regression
```{r}
lg=multinom(is_canceled~.,data=data_train,method="Binomial")
```

```{r}
lgPreds=predict(lg,newdata=data_test)
mean(lgPreds==data_test$is_canceled)
```

```{r}
table(lgPreds,data_test$is_canceled)
```




